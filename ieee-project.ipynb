{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y tensorflow_io","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-06T08:37:27.478461Z","iopub.execute_input":"2023-06-06T08:37:27.479386Z","iopub.status.idle":"2023-06-06T08:37:30.592777Z","shell.execute_reply.started":"2023-06-06T08:37:27.479347Z","shell.execute_reply":"2023-06-06T08:37:30.591497Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found existing installation: tensorflow-io 0.31.0\nUninstalling tensorflow-io-0.31.0:\n  Successfully uninstalled tensorflow-io-0.31.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow_io","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:37:41.638412Z","iopub.execute_input":"2023-06-06T08:37:41.638874Z","iopub.status.idle":"2023-06-06T08:38:15.220734Z","shell.execute_reply.started":"2023-06-06T08:37:41.638833Z","shell.execute_reply":"2023-06-06T08:38:15.219240Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cf4f433ed70>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tensorflow-io/\u001b[0m\u001b[33m\n\u001b[0m^C\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade tensorflow_io","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:35:37.819940Z","iopub.execute_input":"2023-06-06T06:35:37.820272Z","iopub.status.idle":"2023-06-06T06:38:06.553588Z","shell.execute_reply.started":"2023-06-06T06:35:37.820250Z","shell.execute_reply":"2023-06-06T06:38:06.551971Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d2107a9ad10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tensorflow-io/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d2107a9b880>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tensorflow-io/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d2107a9bb20>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tensorflow-io/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d2107a9bcd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tensorflow-io/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d2107a9beb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tensorflow-io/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow_io (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow_io\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"IMPORTING THE REQUIRED MODULES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\n#import tensorflow_addons as tfa\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-06T08:38:24.799582Z","iopub.execute_input":"2023-06-06T08:38:24.799991Z","iopub.status.idle":"2023-06-06T08:38:24.813747Z","shell.execute_reply.started":"2023-06-06T08:38:24.799955Z","shell.execute_reply":"2023-06-06T08:38:24.812520Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}}]},{"cell_type":"markdown","source":"the default variables for the model","metadata":{}},{"cell_type":"code","source":"IMAGE_PATH = '..//input//chinese-mnist//data//data//'\nIMAGE_WIDTH = 64\nIMAGE_HEIGHT = 64\nIMAGE_CHANNELS = 1\nRANDOM_STATE = 2022\nTEST_SIZE = 0.2\nVAL_SIZE = 0.2\nCONV_2D_DIM_1 = 16\nCONV_2D_DIM_2 = 16\nCONV_2D_DIM_3 = 32\nCONV_2D_DIM_4 = 64\nMAX_POOL_DIM = 2\nKERNEL_SIZE = 3\nBATCH_SIZE = 64\nNO_EPOCHS = 15\nDROPOUT_RATIO = 0.4\nPATIENCE = 5\nVERBOSE = 1","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:38:35.815893Z","iopub.execute_input":"2023-06-06T08:38:35.816304Z","iopub.status.idle":"2023-06-06T08:38:35.823830Z","shell.execute_reply.started":"2023-06-06T08:38:35.816274Z","shell.execute_reply":"2023-06-06T08:38:35.822326Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"reading the csv file in the data set","metadata":{}},{"cell_type":"code","source":"main_data=pd.read_csv('..//input//chinese-mnist//chinese_mnist.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:38:47.677512Z","iopub.execute_input":"2023-06-06T08:38:47.677937Z","iopub.status.idle":"2023-06-06T08:38:47.720437Z","shell.execute_reply.started":"2023-06-06T08:38:47.677904Z","shell.execute_reply":"2023-06-06T08:38:47.719323Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"creating a function to automatically return the file name when a row of a dataframe(table) is given","metadata":{}},{"cell_type":"code","source":"def create_file_name(x):\n    \n    file_name = f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:38:51.656938Z","iopub.execute_input":"2023-06-06T08:38:51.657384Z","iopub.status.idle":"2023-06-06T08:38:51.662273Z","shell.execute_reply.started":"2023-06-06T08:38:51.657349Z","shell.execute_reply":"2023-06-06T08:38:51.661072Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":".apply function to apply the above to all rows of the file column...actually a new column file is created and all files names will be added..","metadata":{}},{"cell_type":"code","source":"main_data[\"file\"] = main_data.apply(create_file_name, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:38:57.657857Z","iopub.execute_input":"2023-06-06T08:38:57.658274Z","iopub.status.idle":"2023-06-06T08:38:57.917683Z","shell.execute_reply.started":"2023-06-06T08:38:57.658242Z","shell.execute_reply":"2023-06-06T08:38:57.916486Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"defining a function to return image array by concatenating default image path with each file name","metadata":{}},{"cell_type":"code","source":"def read_image_sizes(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    return list(image.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:39:01.536518Z","iopub.execute_input":"2023-06-06T08:39:01.536927Z","iopub.status.idle":"2023-06-06T08:39:01.542879Z","shell.execute_reply.started":"2023-06-06T08:39:01.536895Z","shell.execute_reply":"2023-06-06T08:39:01.541769Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"stacking the (width,height) of images in a numpy array and then converting it into a dataframe to concatenating to the original dataframe(main_data) ","metadata":{}},{"cell_type":"code","source":"m = np.stack(main_data['file'].apply(read_image_sizes))\nimage_size_df = pd.DataFrame(m,columns=['w','h'])\nmain_data = pd.concat([main_data,image_size_df],axis=1, sort=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:39:05.077210Z","iopub.execute_input":"2023-06-06T08:39:05.077817Z","iopub.status.idle":"2023-06-06T08:41:34.392541Z","shell.execute_reply.started":"2023-06-06T08:39:05.077784Z","shell.execute_reply":"2023-06-06T08:41:34.391261Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Now we CLASSIFY THE CHARACTERS..for this we have to split the dataset into train and  test. further we classify train into train and vaildation . we use RANDOM_STATE and stratify to ensure balanced test sets. stratify actually is used because we are training a model to identify categories.","metadata":{}},{"cell_type":"code","source":"train_df, test_df = train_test_split(main_data, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=main_data[\"code\"].values)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:42:12.378935Z","iopub.execute_input":"2023-06-06T08:42:12.379813Z","iopub.status.idle":"2023-06-06T08:42:12.404186Z","shell.execute_reply.started":"2023-06-06T08:42:12.379765Z","shell.execute_reply":"2023-06-06T08:42:12.403155Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=VAL_SIZE, random_state=RANDOM_STATE, stratify=train_df[\"code\"].values)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:42:16.234694Z","iopub.execute_input":"2023-06-06T08:42:16.235112Z","iopub.status.idle":"2023-06-06T08:42:16.249893Z","shell.execute_reply.started":"2023-06-06T08:42:16.235080Z","shell.execute_reply":"2023-06-06T08:42:16.248904Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"The train-validation split is 80% for training set and 20% for validation set.","metadata":{}},{"cell_type":"markdown","source":"NOW WE ARE GOING TO BUILD THE MODEL","metadata":{}},{"cell_type":"markdown","source":"THIS IS A FUNCTION TO READ IMAGES FROM THEIR FILES QUITE LIKE HOW CV2.IMREAD WORKS AND THEN WE ARE RESIZING THEM USING OUR PREDEFINED WIDTH AND HEIGHT THE CHANNEL IS 1 ENSURING THAT IT IS GRAYSCALE IT FINALLY RETURNS A NUMPY ARRAY","metadata":{}},{"cell_type":"code","source":"def read_image(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    image = skimage.transform.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT, 1), mode='reflect')\n    return image[:,:,:]","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:42:22.826457Z","iopub.execute_input":"2023-06-06T08:42:22.826862Z","iopub.status.idle":"2023-06-06T08:42:22.833263Z","shell.execute_reply.started":"2023-06-06T08:42:22.826829Z","shell.execute_reply":"2023-06-06T08:42:22.832033Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"THIS FUNCTION IS A BIT COMPLEX ONE.FIRSTLY THE DATASET IS A DATAFRAME.WE ARE READING EACH IMAGE FROM THE DF'S FILE COLUMN AND THEN STACKING IT IN X AND VAR IS ACTUALLY THE NAME OF THE COLUMN THAT WE ARE GONNA ENCODE.get_dummies CREATES A NUMPY ARRAY OF DUMMY VARIABLES.","metadata":{}},{"cell_type":"code","source":"def categories_encoder(dataset, var='character'):\n    X = np.stack(dataset['file'].apply(read_image))\n    y = pd.get_dummies(dataset[var], drop_first=False)\n    return X, y","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:42:28.135084Z","iopub.execute_input":"2023-06-06T08:42:28.135459Z","iopub.status.idle":"2023-06-06T08:42:28.140988Z","shell.execute_reply.started":"2023-06-06T08:42:28.135430Z","shell.execute_reply":"2023-06-06T08:42:28.140124Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = categories_encoder(train_df)\nX_val, y_val = categories_encoder(val_df)\nX_test, y_test = categories_encoder(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:42:33.074934Z","iopub.execute_input":"2023-06-06T08:42:33.075843Z","iopub.status.idle":"2023-06-06T08:43:10.624511Z","shell.execute_reply.started":"2023-06-06T08:42:33.075805Z","shell.execute_reply":"2023-06-06T08:43:10.623400Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"NOW THE MAIN PART OF THE PROBELM WHERE WE DESIGN OUR NEURAL NETWORK BY DEFINING LAYERS AND GIVING CERTAIN TRAINING SIZES\n\nThe modes uses 2 Convolutional layers, followed by a MaxPool, a Dropout, then another 2 Convolutional layers and a Dropout. Then follows a Flatten and a Dense layer. We compile the model with Adam optimizer and use a categorical crossentropy loss functions. The metric used is accuracy.\n\nWe are using as well a learning function with variable learning rate (depends on the epoch number).\n\nAt each training epoch, we evaluate the validation error and, based on its evolution, we decide if we stop the training or continue (with a prededined patience factor - i.e. we only stop if validation is not improving for a certain number of steps (we set the patience to 5 steps). If at a certain step the validation error is improving, we save the current model. We then will load the best model and use it for prediction of test set.","metadata":{}},{"cell_type":"code","source":"model=Sequential()\nmodel.add(Conv2D(CONV_2D_DIM_1, kernel_size=KERNEL_SIZE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS), activation='relu', padding='same'))\nmodel.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\nmodel.add(MaxPool2D(MAX_POOL_DIM))\nmodel.add(Dropout(DROPOUT_RATIO))\nmodel.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\nmodel.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\nmodel.add(Dropout(DROPOUT_RATIO))\nmodel.add(Flatten())\nmodel.add(Dense(y_train.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:43:19.479293Z","iopub.execute_input":"2023-06-06T08:43:19.479903Z","iopub.status.idle":"2023-06-06T08:43:19.773811Z","shell.execute_reply.started":"2023-06-06T08:43:19.479870Z","shell.execute_reply":"2023-06-06T08:43:19.772659Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"The model consists of a sequence of convolutional layers, max pooling layers, dropout layers, and dense layers.\n\nThe convolutional layers extract features from the input image. The Conv2D() layer creates a convolutional layer with CONV_2D_DIM_1 filters of size KERNEL_SIZE. The activation='relu' argument specifies that the ReLU activation function will be used. The padding='same' argument specifies that the output of the convolutional layer will have the same width and height as the input image.\nThe max pooling layers downsample the feature maps extracted by the convolutional layers. The MaxPool2D() layer creates a max pooling layer with a pool size of MAX_POOL_DIM.\nThe dropout layers randomly drop out neurons during training, which helps to prevent overfitting. The Dropout() layer with a rate of DROPOUT_RATIO will randomly drop out DROPOUT_RATIO of the neurons in the layer.\nThe dense layers classify the features extracted by the convolutional layers. The Dense() layer creates a dense layer with y_train.columns.size neurons. The activation='softmax' argument specifies that the softmax activation function will be used, which will output a probability distribution over the classes.\nThe compile() method configures the model for training. The optimizer='adam' argument specifies that the Adam optimizer will be used. The loss='categorical_crossentropy' argument specifies that the categorical cross-entropy loss function will be used. The metrics=['accuracy'] argument specifies that the accuracy metric will be used to evaluate the model.\nOnce the model is compiled, it can be trained using the fit() method. The fit() method takes the training data and labels as input and trains the model for a specified number of epochs. The model can then be evaluated on the validation data using the evaluate() method. The evaluate() method returns the loss and accuracy on the validation data. The model can then be used to make predictions on new data using the predict() method.","metadata":{}},{"cell_type":"markdown","source":" FURTHER IMPROVING THE PERFORMANCE\n \n The annealing scheduler reduces the learning rate of the model over time. The LearningRateScheduler() class creates a learning rate scheduler that uses a lambda function to calculate the new learning rate for each epoch. The lambda function takes the epoch number as input and returns a new learning rate. In this case, the new learning rate is calculated as 1e-3 * 0.99 ** (x+NO_EPOCHS), where x is the epoch number and NO_EPOCHS is the number of epochs.\nThe early stopper stops training if the loss on the validation set does not improve for a specified number of epochs. The EarlyStopping() class creates an early stopper that monitors the loss on the validation set and stops training if the loss does not improve for patience epochs.\nThe model checkpointer saves the model to a file if the model achieves a new best accuracy on the validation set. The ModelCheckpoint() class creates a model checkpointer that saves the model to a file if the model achieves a new best accuracy on the validation set. The monitor='val_accuracy' argument specifies that the model checkpointer will monitor the accuracy on the validation set. The verbose=VERBOSE argument specifies that the model checkpointer will print a message when the model is saved. The save_best_only=True argument specifies that the model checkpointer will only save the model if it achieves a new best accuracy. The save_weights_only=True argument specifies that the model checkpointer will only save the weights of the model, not the entire model.\nThese callbacks can be used to improve the performance of the model by preventing overfitting and by saving the model when it achieves a new best accuracy.","metadata":{}},{"cell_type":"code","source":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.99 ** (x+NO_EPOCHS))\nearlystopper = EarlyStopping(monitor='loss', patience=PATIENCE, verbose=VERBOSE)\ncheckpointer = ModelCheckpoint('best_model.h5',\n                                monitor='val_accuracy',\n                                verbose=VERBOSE,\n                                save_best_only=True,\n                                save_weights_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:43:27.835617Z","iopub.execute_input":"2023-06-06T08:43:27.836042Z","iopub.status.idle":"2023-06-06T08:43:27.842394Z","shell.execute_reply.started":"2023-06-06T08:43:27.835996Z","shell.execute_reply":"2023-06-06T08:43:27.841074Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"TRAINING THE MODEL\n\n\nbatch_size: The number of samples to process at a time.\nepochs: The number of times to iterate over the entire training data.\nverbose: The verbosity level, with 0 being silent and 1 being the most verbose.\nvalidation_data: A tuple of (X_val, y_val) data to use for validation.\ncallbacks: A list of callbacks to use during training.\nThe callbacks are:\n\nearlystopper: A callback that will stop training early if the validation loss stops improving.\ncheckpointer: A callback that will save the model to disk at regular intervals.\nannealer: A callback that will gradually reduce the learning rate as training progresses.\n\nHere is an explanation of each of the parameters:\n\nbatch_size: The batch size is the number of samples that are processed at a time. A larger batch size can improve performance, but it can also use more memory.\nepochs: The number of epochs is the number of times that the model will be trained on the entire training data. A larger number of epochs will generally result in a better model, but it will also take longer to train.\nverbose: The verbose level controls how much output is generated during training. A value of 0 will suppress all output, while a value of 1 will print out the progress of training.\nvalidation_data: The validation data is a set of data that is not used for training. The model is evaluated on the validation data after each epoch, and this information is used to determine when to stop training.\ncallbacks: Callbacks are functions that are called during training. They can be used to perform various tasks, such as saving the model to disk, reducing the learning rate, or sending metrics to a monitoring service.","metadata":{}},{"cell_type":"code","source":"train_model  = model.fit(X_train, y_train,\n                  batch_size=BATCH_SIZE,\n                  epochs=NO_EPOCHS,\n                  verbose=1,\n                  validation_data=(X_val, y_val),\n                  callbacks=[earlystopper, checkpointer, annealer])","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:43:32.711039Z","iopub.execute_input":"2023-06-06T08:43:32.711458Z","iopub.status.idle":"2023-06-06T08:54:37.434968Z","shell.execute_reply.started":"2023-06-06T08:43:32.711424Z","shell.execute_reply":"2023-06-06T08:54:37.434155Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1/15\n150/150 [==============================] - ETA: 0s - loss: 1.8667 - accuracy: 0.4263\nEpoch 1: val_accuracy improved from -inf to 0.61958, saving model to best_model.h5\n150/150 [==============================] - 46s 298ms/step - loss: 1.8667 - accuracy: 0.4263 - val_loss: 1.2979 - val_accuracy: 0.6196 - lr: 8.6006e-04\nEpoch 2/15\n150/150 [==============================] - ETA: 0s - loss: 0.9474 - accuracy: 0.7029\nEpoch 2: val_accuracy improved from 0.61958 to 0.78542, saving model to best_model.h5\n150/150 [==============================] - 45s 297ms/step - loss: 0.9474 - accuracy: 0.7029 - val_loss: 0.7082 - val_accuracy: 0.7854 - lr: 8.5146e-04\nEpoch 3/15\n150/150 [==============================] - ETA: 0s - loss: 0.6009 - accuracy: 0.8087\nEpoch 3: val_accuracy improved from 0.78542 to 0.84208, saving model to best_model.h5\n150/150 [==============================] - 44s 296ms/step - loss: 0.6009 - accuracy: 0.8087 - val_loss: 0.5127 - val_accuracy: 0.8421 - lr: 8.4294e-04\nEpoch 4/15\n150/150 [==============================] - ETA: 0s - loss: 0.4607 - accuracy: 0.8526\nEpoch 4: val_accuracy improved from 0.84208 to 0.86333, saving model to best_model.h5\n150/150 [==============================] - 44s 295ms/step - loss: 0.4607 - accuracy: 0.8526 - val_loss: 0.4266 - val_accuracy: 0.8633 - lr: 8.3451e-04\nEpoch 5/15\n150/150 [==============================] - ETA: 0s - loss: 0.3709 - accuracy: 0.8813\nEpoch 5: val_accuracy improved from 0.86333 to 0.89958, saving model to best_model.h5\n150/150 [==============================] - 45s 300ms/step - loss: 0.3709 - accuracy: 0.8813 - val_loss: 0.3455 - val_accuracy: 0.8996 - lr: 8.2617e-04\nEpoch 6/15\n150/150 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.8952\nEpoch 6: val_accuracy improved from 0.89958 to 0.91583, saving model to best_model.h5\n150/150 [==============================] - 44s 295ms/step - loss: 0.3133 - accuracy: 0.8952 - val_loss: 0.3210 - val_accuracy: 0.9158 - lr: 8.1791e-04\nEpoch 7/15\n150/150 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.9042\nEpoch 7: val_accuracy improved from 0.91583 to 0.91708, saving model to best_model.h5\n150/150 [==============================] - 44s 296ms/step - loss: 0.2748 - accuracy: 0.9042 - val_loss: 0.3168 - val_accuracy: 0.9171 - lr: 8.0973e-04\nEpoch 8/15\n150/150 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.9174\nEpoch 8: val_accuracy improved from 0.91708 to 0.92667, saving model to best_model.h5\n150/150 [==============================] - 44s 292ms/step - loss: 0.2372 - accuracy: 0.9174 - val_loss: 0.3094 - val_accuracy: 0.9267 - lr: 8.0163e-04\nEpoch 9/15\n150/150 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.9245\nEpoch 9: val_accuracy did not improve from 0.92667\n150/150 [==============================] - 43s 289ms/step - loss: 0.2208 - accuracy: 0.9245 - val_loss: 0.2901 - val_accuracy: 0.9233 - lr: 7.9361e-04\nEpoch 10/15\n150/150 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9367\nEpoch 10: val_accuracy improved from 0.92667 to 0.93542, saving model to best_model.h5\n150/150 [==============================] - 44s 292ms/step - loss: 0.1885 - accuracy: 0.9367 - val_loss: 0.2400 - val_accuracy: 0.9354 - lr: 7.8568e-04\nEpoch 11/15\n150/150 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9416\nEpoch 11: val_accuracy improved from 0.93542 to 0.94792, saving model to best_model.h5\n150/150 [==============================] - 44s 294ms/step - loss: 0.1722 - accuracy: 0.9416 - val_loss: 0.2129 - val_accuracy: 0.9479 - lr: 7.7782e-04\nEpoch 12/15\n150/150 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.9460\nEpoch 12: val_accuracy improved from 0.94792 to 0.94833, saving model to best_model.h5\n150/150 [==============================] - 44s 293ms/step - loss: 0.1595 - accuracy: 0.9460 - val_loss: 0.2012 - val_accuracy: 0.9483 - lr: 7.7004e-04\nEpoch 13/15\n150/150 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9574\nEpoch 13: val_accuracy did not improve from 0.94833\n150/150 [==============================] - 44s 293ms/step - loss: 0.1289 - accuracy: 0.9574 - val_loss: 0.2515 - val_accuracy: 0.9350 - lr: 7.6234e-04\nEpoch 14/15\n150/150 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9558\nEpoch 14: val_accuracy improved from 0.94833 to 0.95333, saving model to best_model.h5\n150/150 [==============================] - 44s 295ms/step - loss: 0.1318 - accuracy: 0.9558 - val_loss: 0.2023 - val_accuracy: 0.9533 - lr: 7.5472e-04\nEpoch 15/15\n150/150 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9600\nEpoch 15: val_accuracy did not improve from 0.95333\n150/150 [==============================] - 44s 295ms/step - loss: 0.1201 - accuracy: 0.9600 - val_loss: 0.2150 - val_accuracy: 0.9475 - lr: 7.4717e-04\n","output_type":"stream"}]},{"cell_type":"markdown","source":"NOW WE OBTAIN THE RESULTS BY EVALUVATING THE MODEL WE GET THE LOSS AND ACCURACY AS A TUPLE","metadata":{}},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\nprint('\\n'*5)\nprint(f'Test accuracy% : {score[1]*100:.2f} %')\nprint(f'Test loss% : {score[0]*100:.2f} %')","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:55:59.420633Z","iopub.execute_input":"2023-06-06T08:55:59.421098Z","iopub.status.idle":"2023-06-06T08:56:02.227297Z","shell.execute_reply.started":"2023-06-06T08:55:59.421061Z","shell.execute_reply":"2023-06-06T08:56:02.226004Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Test loss: 0.2088829129934311\nTest accuracy: 0.9480000138282776\n\n\n\n\n\n\nTest accuracy% : 94.80 %\nTest loss% : 20.89 %\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}